{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph_Neural_Networks (1).ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/graph-neural-networks/blob/master/Graph_Neural_Networks_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSV9B0TwON5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spektral"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAIT3l-9SoHM",
        "colab_type": "text"
      },
      "source": [
        "## Node classification on citation networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t5PqprFSrrO",
        "colab_type": "text"
      },
      "source": [
        "In this example, we will build a simple Graph Convolutional Network for semi-supervised classification of nodes.\n",
        "This is a simple but challenging task that consists of classifying text documents in a citation network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4IjrNDdS_-s",
        "colab_type": "text"
      },
      "source": [
        "In this type of graph, each node represents a document and is associated to a binary bag-of-words attribute (1 if a given word appears in the text, 0 otherwise). If a document cites another, then there exist an undirected edge between the two corresponding nodes. Finally, each node has a class label that we want to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTlSiZQOTNpB",
        "colab_type": "text"
      },
      "source": [
        "This is a transductive learning setting, where we observe all of the nodes and edges at training time, but only a fraction of the labels. The goal is to learn to predict the missing labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukP3h6Q-TTaM",
        "colab_type": "text"
      },
      "source": [
        "### The datasets.citation module of Spektral lets you download and load three popular citation datasets (Cora, Citeseer and Pubmed) in one line of code. For instance, loading the Cora dataset is as simple as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxYDD5LjQmsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spektral.datasets import citation\n",
        "A, X, y, train_mask, val_mask, test_mask = citation.load_data('cora')\n",
        "\n",
        "N = A.shape[0]\n",
        "F = X.shape[-1]\n",
        "n_classes = y.shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjJGJy0SfJDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBJ-tbNqfMuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRWrBFncYZ1l",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This will load the network's adjacency matrix A as a Scipy sparse matrix of shape (N, N), the node features X of shape (N, F), and the labels y of shape (N, n_classes). The loader will also return some boolean masks to know which nodes belong to the training, validation and test sets (train_mask, val_mask, test_mask).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uoe4nlIYp9o",
        "colab_type": "text"
      },
      "source": [
        "## Creating a GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CluBHd5-YsgK",
        "colab_type": "text"
      },
      "source": [
        "## To create a GCN, we will use the GraphConv layer and the functional API of Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc6cjNflYblY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spektral.layers import GraphConv\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iQn5EXxZ9Qn",
        "colab_type": "text"
      },
      "source": [
        "Building the model is no different than building any Keras model, but we will need to provide multiple inputs (X and A) to the GraphConv layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5T8GBPPZ92y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_in = Input(shape=(F, ))\n",
        "A_in = Input((N, ), sparse=True)\n",
        "\n",
        "X_1 = GraphConv(16, 'relu')([X_in, A_in])\n",
        "X_1 = Dropout(0.5)(X_1)\n",
        "X_2 = GraphConv(n_classes, 'softmax')([X_1, A_in])\n",
        "\n",
        "model = Model(inputs=[X_in, A_in], outputs=X_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-akqFtRaKHO",
        "colab_type": "text"
      },
      "source": [
        "And that's it. We just built our first GNN in Spektral and Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E9OTpXaaQMk",
        "colab_type": "text"
      },
      "source": [
        "### Note how we used the familiar API of Keras to create the GCN layers, as well as the standard Dropout layer to regularize our model. All features of Keras are also supported by Spektral (including initializers, regularizers, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jU1E7ycahfu",
        "colab_type": "text"
      },
      "source": [
        "Notice how we defined the Input layers of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvp0Zs5FanBl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*    As the \"elements\" of our dataset are the node themselves, we are telling       Keras to consider each node as a separate sample so that the batch axis is implicitly defined as None.\n",
        "*   In other words, a sample of the node attributes will be a vector of shape \n",
        "  (F, ) and a sample of the adjacency matrix will be one row of shape (N, ).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xlLXf3YbDO6",
        "colab_type": "text"
      },
      "source": [
        "## Training the GNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP0MFf8sbH2I",
        "colab_type": "text"
      },
      "source": [
        "When training GCN, we have to pre-process the adjacency matrix to \n",
        "*  Add self-loops and \n",
        "*  Scale the weights of a node's connections according to its degree. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp3p3nuubXLS",
        "colab_type": "text"
      },
      "source": [
        "Some layers in **Spektral** require a different type of pre-processing in order to work correctly, and some work out-of-the-box on the binary A. The pre-processing required by each layer is available as a static class method preprocess()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO8scFFabdoL",
        "colab_type": "text"
      },
      "source": [
        "In our example, the pre-processing required by GCN is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax-0aKZIaCLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = GraphConv.preprocess(A).astype('f4')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysuwIJo1bhbX",
        "colab_type": "text"
      },
      "source": [
        "And that's all! What's left now for us is to compile and train our model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZhIqnzqbiJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              weighted_metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Ux19VSbn5Q",
        "colab_type": "text"
      },
      "source": [
        "Note that we used the **weighted_metrics** argument instead of the usual metrics. This is due to the particular **semi-supervised** problem that we are dealing with, and has to do with the boolean masks that we loaded earlier (more on that later).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWMXBmnb3mG",
        "colab_type": "text"
      },
      "source": [
        "We can now train the model using the native fit() method of Keras:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFbD3TMhbpNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data\n",
        "X = X.toarray()\n",
        "A = A.astype('f4')\n",
        "validation_data = ([X, A], y, val_mask)\n",
        "\n",
        "# Train model\n",
        "model.fit([X, A], y,\n",
        "          sample_weight=train_mask,\n",
        "          validation_data=validation_data,\n",
        "          batch_size=N,\n",
        "          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEkJH7-xb_wO",
        "colab_type": "text"
      },
      "source": [
        "## Few Things to Note here\n",
        "*   We have set batch_size=N and shuffle=False.\n",
        "*   This is because the default behaviour of Keras is to split the data into     batches of 32 and shuffle the samples at each epoch. \n",
        "\n",
        "*    However, shuffling the adjacency matrix along one axis and not the other means that row i will represent a different node than column i.\n",
        "*   At the same time, if we split the graph into batches we may end up in a situation where we need to use a node attribute that is not part of the batch.\n",
        "\n",
        "\n",
        "*   The only solution is to take all the node features at the same time, hence batch_size=N.\n",
        "\n",
        "### *   Finally, we used train_mask and val_mask as sample_weight.m\n",
        "\n",
        "*   This means that, during training, the training nodes will have a weight of 1 and the validation nodes will have a weight of 0. Then, in validation, we will set the training nodes to have a weight of 0 and the validation nodes to have a weight of 1.\n",
        "*   This is all that we need to do to differentiate between training and test data. See how the model takes as input the full X, A, and y for both training and valdation?\n",
        "\n",
        "\n",
        "### *   The only thing that changes is the mask.\n",
        "### *   This is also why we used the weighted_metrics keyword when compiling the model, so that our accuracy is calculated only on the correct nodes at each phase.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qipr6mGleoe8",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqvyOZI5es3W",
        "colab_type": "text"
      },
      "source": [
        "Once again, evaluation is done in vanilla Keras. We just have to keep in mind the same considerations about batching that we did for training (note that in model.evaluate() by default shuffle=False):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY7nQwLCeffi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model\n",
        "eval_results = model.evaluate([X, A],\n",
        "                              y,\n",
        "                              sample_weight=test_mask,\n",
        "                              batch_size=N)\n",
        "print('Done.\\n'\n",
        "      'Test loss: {}\\n'\n",
        "      'Test accuracy: {}'.format(*eval_results))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}